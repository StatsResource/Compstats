

%--------------------------%
% - Interpreting Confidence Interval. READY
% - Introducing Hypothesis testing. READY
% - The Null and Alternative Hypotheses.
% - Computing Test Statistics.
% - P-values
% - Critical values
% - Decision Rules
% - One Side ad Two Sided tests
% - Type I and Type 2 Error
% - The Paired T test.
%----------------------------------------------------------------------------------------------------%




%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Hypothesis Testing : Revision from Last Class}
\large
\begin{itemize} \item
A hypothesis test is a method of making decisions using experimental data. \item A result is called \textbf{\emph{statistically significant}} if it is unlikely to have occurred by chance.


%\item A statistical test procedure is comparable to a trial where a defendant is considered innocent as long as his guilt is not proven.\item  The prosecutor tries to prove the guilt of the defendant. Only when there is enough charging evidence the defendant is condemned.
%\end{itemize}

%


%--------------------------------------------------------------------------------------------------------------------------%
%
%\noindent \textbf{Hypothesis tests (Null and Alternative Hypotheses) }
%\large
%The phrase "test of significance" was coined by Ronald Fisher;
%"Critical tests of this kind may be called tests of significance, and when such tests are available we may discover whether a second sample is or is not significantly different from the first." \\
%\begin{itemize}
\item The null hypothesis (which we will denoted $H_0$) is an hypothesis about a population parameter, such as the population mean $\mu$. \item The purpose of hypothesis testing is to test the viability of the null hypothesis in the light of experimental data. \item The alternative hypothesis $H_1$ expresses the exact opposite of the null hypothesis. \item Depending on the data, the null hypothesis either will or will not be \textbf{rejected} as a viable possibility in favour of the alternative hypothesis.
\end{itemize}









%----------------------------------------------------------------------------------------------------%
{
\noindent \textbf{Writing the Null Hypothesis}
%In statistics, a hypothesis is a claim or statement made around a property of a population.
%A hypothesis test (also called a test of significance) is a standard procedure for testing a claim about that %property.
\begin{itemize}
%\item The null hypothesis is denoted $H_0$.
\item It will often express it's argument in the form of a mathematical relation, with a written description of the hypothesis (we will do it this way).
\item $H_0$ will always refer to the population parameter ( i.e. never the observed value) and must contain a condition of equality. (i.e. ` = ' , `$ \leq$' or `$\geq$')
\end{itemize}
}


%----------------------------------------------------------------------------------------------------%
{
\noindent \textbf{Number of Tails}
Inference Procedures are either \textbf{One-tailed} or \textbf{Two-Tailed}.


 Confidence Intervals are always two-tailed . Hypothesis tests can either be one-tailed or two-tailed. It is important to know how determine correctly the number of tails.
\begin{itemize}
\item The alternative hypothesis indicates the number of tails.
\item A rule of thumb is to consider how many alternative to the $H_0$ is offered by $H_1$.
\item When $H_1$ includes either of these relational operators;`$>$' ,`$<$' , only one alternative is offered.
\item When $H_1$ includes the $\neq$ relational operators, two alternatives are offered (i.e.`$>$' or `$<$').
\end{itemize}
}


%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Significance Level ($\alpha$)}

\begin{itemize}
\item In hypothesis testing, the significance level $\alpha$ is the criterion used for rejecting the null hypothesis. \item The significance level is used in hypothesis testing as follows: First, the difference between the result (i.e. observed statistic or point estimate)  of the experiment and the \textbf{null value} \item The null value is the expected value of this statistic, assuming that the null hypothesis is true), is determined.(i.e.we denote this  \textbf{\textit{Observed - Null}}). \item Then, assuming the null hypothesis is true, the probability of a difference that large or larger is computed . \item Finally, this probability is compared to the significance level.\item  If the probability is less than or equal to the significance level, then the null hypothesis is rejected and the outcome is said to be statistically significant.
\end{itemize}


%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Hypothesis Testing}
The inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
\bigskip
This means that:
\begin{itemize}
\item[(1)] a very unlikely event occurred or
\item[(2)] the null hypothesis is false.
\end{itemize}
\bigskip
The inference usually made is that the null hypothesis is false. Importantly it doesn't prove the null hypothesis to be false.

%--------------------------------------------------------------------------------------------------------------------------%



\noindent \textbf{Significance (Die Throw Example)}
\begin{itemize}
\item Suppose that the outcome of the die throw experiment was a sum of 401. In previous lectures, a simulation study found that only in approximately $1.75\%$ of cases would a fair die yield this result.
\item However, in the case of a crooked die (i.e. one that favours high numbers) this result would not be unusual.
\item A reasonable interpretation of this experiment is that the die is crooked, but importantly the experiment doesn't prove it one way or the other.
\item We will discuss the costs of making a wrong decision later (Type I and Type II errors).
\end{itemize}

%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Significance Level}

\begin{itemize}
\item Traditionally, experimenters have used either the 0.05 level (sometimes called the 5\% level) or the 0.01 level (1\% level), although the choice of levels is largely subjective.  \item The lower the significance level, the more the data must diverge from the null hypothesis to be significant. \item Therefore, the 0.01 level is more conservative than the 0.05 level. \item The Greek letter alpha ($\alpha$) is sometimes used to indicate the significance level. \item We will $\alpha =0.05$ in this module. (i.e. 5\%) \end{itemize}


%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Hypothesis Testing and p-values}
\begin{itemize}
\item In hypothesis tests, the difference between the observed value and the parameter value specified by $H_0$ is computed and the probability of obtaining a difference this large or large is calculated.
\item The probability of obtaining data as extreme, or more extreme, than the expected value under the null hypothesis is called the \textbf{\emph{p-value}}.
\item There is often confusion about the precise meaning of the p-value probability computed in a significance test. It is not the probability of the null hypothesis itself.
\item Thus, if the probability value is $0.0175$, this does not mean that the probability that the null hypothesis is either true or false is $0.0175$.
\item It means that the probability of obtaining data as different or more different from the null hypothesis as those obtained in the experiment is $0.0175$.
\end{itemize}


%---------------------------------------------------------------------------------------------%

{
\noindent \textbf{Significance Level}

\begin{itemize}
\item The significance level of a statistical hypothesis test is a fixed probability of wrongly rejecting the null hypothesis $H_0$, if it is in fact true.

\item Equivalently, the significance level (denoted by $\alpha$) is the probability that the test statistics will fall into the \textbf{\emph{critical region}}, when the null hypothesis is actually true. ( We will discuss the critical region shortly).

\item Common choices for $\alpha$ are $0.05$ and $0.01$
\end{itemize}
}

%--------------------------%


\noindent \textbf{The Hypothesis Testing Procedure }
We will use both of the following four step procedures for hypothesis testing. The level of significance must be determined in advance.

\textbf{Procedure 1}\\
The first procedures is as follows:

\begin{itemize}
\item Formally write out the null and alternative hypotheses (already described).
\item Compute the \emph{\textbf{test statistic}} - a standardized value of the numerical outcome of an experiment.
\item Compute the p-value for that test statistic.
\item Make a decision based on the p-value. (smaller than $\alpha$ or $\alpha/2$? - reject null)
\end{itemize}
(We will re-visit this approach later in the course).


%--------------------------%


\noindent \textbf{The Hypothesis Testing Procedure }
The second procedures is very similar to the first, but is more practicable for written exams, so we will use this one also. The first two steps are the same.
\textbf{Procedure 2}\\
\begin{itemize}
\item Formally write out the null and alternative hypotheses (already described).
\item Compute the test statistic
\item Determine the \emph{\textbf{critical value}} (described shortly)
\item Make a decision based on the critical value. (We call this step the \textbf{decision rule} step, and shall discuss it in depth shortly).
\end{itemize}
(We will mostly use this approach to hypothesis testing).



%------------------------------------------------%

\noindent \textbf{Test Statistics}
\begin{itemize}
\item A test statistic is a quantity calculated from our sample of data. Its value is used to decide whether or not the null hypothesis should be rejected in our hypothesis test.
\item The choice of a test statistic will depend on the assumed probability model and the hypotheses under question.
    \item The general structure of a test statistic is
\[ \mbox{TS}  = {\mbox{Observed Value} - \mbox{Null Value}  \over \mbox{Std. Error}}\]
\item Recall: The ``Null Value" is the expected value, assuming that the null hypothesis is true.
\end{itemize}

%----------------------------------------------%


\noindent \textbf{The Test Statistic}
\begin{itemize}

\item In our dice experiment, we observed a value of 401. Under the null hypothesis, the expected value was 350.
\item The standard error is of the same form as for confidence intervals. $s \over \sqrt{n}$.
\item (For this experiment the standard error is 17.07).
\item The test statistic is therefore \[ \mbox{TS}  = {401 - 350  \over 17.07} = 2.99 \]
\end{itemize}


%--------------------------%


\noindent \textbf{The Critical Value}


\begin{itemize}
\item The critical value(s) for a hypothesis test is a threshold to which the value of the test statistic in sample is compared to determine whether or not the null hypothesis is rejected.
\item The critical value for any hypothesis test depends on the significance level at which the test is carried out, and whether the test is one-sided or two-sided.
\item The critical value is determined the exact same way as quantiles for confidence intervals; using Murdoch Barnes table 7.


\end{itemize}



\noindent \textbf{Critical Region}
In class : graphical representation of material on the black-board is scheduled here.



\textbf{Important:}
A critical value is any value that separates the critical region ( where we reject the null hypothesis) for thatvalues of the test statistic that do not lead to a rejection of the null hypothesis.

%--------------------------%



\noindent \textbf{One Tailed Hypothesis test}
\begin{itemize}
\item A one-sided test is a statistical hypothesis test in which the values for which we can reject the null hypothesis, $H_0$ are located entirely in one tail of the probability distribution (either the upper tail, or the lower tail, but not both).

\item In other words, the critical region for a one-sided test is the set of values beyond than the critical value for the test

\item A one-sided test is also referred to as a one-tailed test of significance.

\item A rule of thumb is to consider the alternative hypothesis.  If only one alternative is offered by $H_1$ (i.e. a $`<'$ or a $`>'$ is present, then it is a one tailed test.)
\item (When computing quantiles from Murdoch Barnes table 7, we set $k=1$)
\end{itemize}



%---------------------------------------------------------------------------------------------%

\noindent \textbf{Two Tailed Hypothesis test}
\begin{itemize}
\item
A two-sided test is a statistical hypothesis test in which the values for which we can reject the null hypothesis, $H_0$ are located in both tails of the probability distribution, on an equal basis.

\item A two-sided test is also referred to as a two-tailed test of significance.
\item A rule of thumb is to consider the alternative hypothesis.  If only one alternative is offered by $H_1$ (i.e. a $`\neq'$ is present, then it is a two tailed test.)
\item (When computing quantiles from Murdoch Barnes table 7, we set $k=2$)

\end{itemize}



%--------------------------%



\noindent \textbf{Determining the Critical value}
\begin{itemize} \item The critical value for a hypothesis test is a threshold to which the value of the test statistic in a sample is compared to determine whether or not the null hypothesis is rejected.

\item The critical value for any hypothesis test depends on the significance level at which the test is carried out, and whether the test is one-sided or two-sided.
\end{itemize}



%--------------------------%



\noindent \textbf{Determining the Critical value}
\begin{itemize}
\item A pre-determined level of significance $\alpha$ must be specified. Usually it is set at 5\% (0.05).
\item The number of tails must be known. ( $k$ is either 1 or 2).
\item Sample size will be also be an issue. We must decide whether to use $n-1$ degrees of freedom or $\infty$ degrees of freedom, depending on the sample size in question.
\item The manner by which we compute critical value is identical to the way we compute quantiles.We will consider this in more detail during tutorials.
\item For the time being we will use 1.96 as a critical value.
\end{itemize}


%------------------------------------------%


\noindent \textbf{Decision Rule:  The Critical Region}
\begin{itemize}
\item The critical region CR (or rejection region RR) is a set of values of the test statistic for which the null hypothesis is rejected in a hypothesis test. \item That is, the sample space for the test statistic is partitioned into two regions; one region (the critical region) will lead us to reject the null hypothesis $H_0$, the other will not.

\item A test statistic is in the critical region if the absolute value of the test statistic is greater than the critical value.
    \item So, if the observed value of the test statistic is a member of the critical region, we conclude ``Reject $H_0$"; if it is not a member of the critical region then we conclude "Do not reject $H_0$".
\item (Demonstration on BlackBoard).
\end{itemize}




\noindent \textbf{Critical Region }

Remark: the absolute value fuction of some value x is denoted $|x|$. It is the magnitude of the value without consideration of whether the value is positive or negative.


\begin{itemize}
\item Let TS denote Test Statistic and CV denoted Critical Value.
\item $|TS| > CV$ Then we reject null hypothesis.
\item $|TS| \leq CV$ Then we \textbf{fail to reject} null hypothesis.

\item For our die-throw example; TS = 2.99, CV = 1.96.
\item Here $|2.99| > 1.96$ we reject the null hypothesis that the die is fair.
\item Consider this in the context of ``proof". (More on this in next class)
\end{itemize}





\noindent \textbf{Performing a Hypothesis test}
\textbf{Important}
To summarize: a hypothesis test can be considered as a four step process
\begin{itemize}
\item[1] Formally writing out the null and alternative hypothesis.
\item[2] Computing the test statistic.
\item[3] Determining the critical value.
\item[4] Using the decision rule.
\end{itemize}


%----------------------------------------------------------------------------------------------------%

{
\noindent \textbf{Conclusions in Hypothesis Testing}
\textbf{Important}
\begin{itemize}
\item We always test the null hypothesis.
\item We either \textbf{\emph{reject}} the null hypothesis, or
\item We \textbf{\emph{ fail to reject}} the null hypothesis.
\item our conclusion is always one of these two.
\end{itemize}
}


%--------------------------%
{
\noindent \textbf{p-values}
(Mentioned Previously, but discussed again)
\begin{itemize}
\item The p-value (or P-value or probability value) is the probability of getting a value of the test statistic that is at least as extreme as the one representing the sample data, assuming the null hypothesis is true.
\item The null hypothesis is rejected if the p-value is very small, such as less than 0.05.
\item When performing an inference procedure on a computer, it is much more common to use the p-value as a basis for decision, rather than the critical value
\end{itemize}
}



%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{p-values}
\begin{itemize}
\item The less likely the obtained results (or more extreme results) under the null hypothesis, the more confident one should be that the null hypothesis is false.The null hypothesis should not be rejected once and for all. The possibility that it was falsely rejected is always present, and, all else being equal, the lower the p-value, the lower this possibility.
\item According to this view, research reports should not contain the p-value, only whether or not the values were significant (at or below the significance level).
\item
However it is much more reasonable to just report the p-values. That way each reader can make up his or her mind about just how convinced they are that the null hypothesis is false.
\end{itemize}


\noindent \textbf{Hypothesis Testing}
\large
Recall: the inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
\bigskip
This means that:
\begin{itemize}\item [(1)] a very unlikely event occurred or
\item[(2)] the null hypothesis is false. \end{itemize}
The inference usually made is that the null hypothesis is false. Importantly it doesnt prove the null hypothesis to be false.

%-------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
There are two kinds of errors that can be made in hypothesis testing:
\begin{itemize}
\item[(1)] a true null hypothesis can be incorrectly rejected
\item[(2)] a false null hypothesis can fail to be rejected.
\end{itemize}
The former error is called a \textbf{\emph{Type I error}} and the latter error is called a \textbf{\emph{Type II error}}. \\ \bigskip
The probability of Type I error is always equal to the level of significance $\alpha$ (alpha) that is used as the standard for rejecting the null hypothesis .

%---------------------------------------------------------------------------%

\noindent \textbf{Type II Error}
\begin{itemize}

\item The probability of a Type II error is designated by the Greek letter beta ($\beta$).
\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost.
\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. \item Therefore, Type I errors are generally considered more serious than Type II errors.
\item
The probability of a Type I error ($\alpha$ ) is set by the experimenter. \item There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. \item However, it increases the chance that a false null hypothesis will not be rejected, thus increasing the likelihood of Type II error.
\item
The Type I error rate is almost always set at 0.05 or at 0.01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the 0.01 level then at the 0.05 level.
\item \textbf{Important} In this module, the significance level $\alpha$ can be assumed to be 0.05, unless explicitly stated otherwise.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
These two types of errors are defined in the table below.
\small
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
&True State: H0 True & True State: H0 False\\\hline
Decision: Reject H0 & Type I error& Correct\\\hline
Decision: Do not Reject H0 & Correct &Type II error\\ \hline
\end{tabular}
\end{center}




%----------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and Type II errors - Die Example}
\begin{itemize}
\item Recall our die throw experiment example.
\item Suppose we perform the experiment twice with two different dice.
\item We don't not know for sure whether or not either of the dice is fair or crooked (favouring high values).
\item Suppose we get a sum of 401 from one die, and 360 from the other.
\end{itemize}


%----------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and Type II errors - Die Example}
\begin{itemize}
\item For our first dice (sum 401), we feel that it is likely that the die is crooked.
\item A Type I error describes the case when in fact that dice was fair, and what happened was just an unusual result.
\item For our second dice (sum 360), we feel that it is likely that the die is fair.
\item A Type II error describes the case when in fact that dice was crooked, favouring high values, and what happened was, again, just an unusual result.
\end{itemize}




\end{document}












%----------------------------------------------------------------------------------------------------%
{
\begin{itemize}
\item $\mu_d$ mean value for the population of differences.
\item $\bar{d}$ mean value for the sample of differences,
\item $s_d$ standard deviation of the differences for the paired sample data.
\item $n$ number of pairs
\end{itemize}


}




%---------------------------------------------------------------------------------------------%
{
\noindent \textbf{The Critical region}
The critical region ( or rejection region ) is the set of all values of the test statistic that causes us to rejec the null hypothesis.

}
{

Test statistics for testing a claim about a mean, when the population variance is known.

\[ Z = {\bar{x}  - \mu \over {\sigma \over \sqrt{n}}} \]
}





%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{P-values}
\large
\begin{itemize}
\item The null hypothesis either is or is not rejected at the previously stated significance level. Thus, if an experimenter originally stated that he or she was using the $\alpha = 0.05$ significance level and p-value was subsequently calculated to be $0.042$, then the person would reject the null hypothesis at the 0.05 level. \item If p-value had been 0.0001 instead of 0.042 then the null hypothesis would still be rejected at the 0.05 significance level.  \item
The experimenter would not have any basis to be more confident that the null hypothesis was false with a p-value of 0.0001 than with a p-value of 0.041. \item Similarly, if the p had been 0.051 then the experimenter would fail to reject the null hypothesis
\end{itemize}


%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\begin{itemize}
\item The probability of a Type I error is designated by the Greek letter alpha ( $\alpha$) and is called the Type I error rate.
\item The probability of a Type II error (the Type II error rate) is designated by the Greek letter beta ( $\beta$ ).
\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost.
\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
\end{itemize}

%---------------------------------------------------------------------------------------------%





\noindent \textbf{Type I and Type II Error}
\begin{itemize}
\item
\item
\item A type II error would occur if it was concluded that the two drugs produced the same effect, i.e. there is no difference between the two drugs on average, when in fact they produced different ones.
\item A type II error is frequently due to sample sizes being too small.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\begin{itemize}
\item
A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. Therefore, Type I errors are generally considered more serious than Type II errors.
 \item
The probability of a Type I error ( ) is called the significance level and is set by the experimenter. There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
\item
Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. However, it increases the chance that a false null hypothesis will not be rejected, thus lowering the likelihood of Type II error.
\item
The Type I error rate is almost always set at .05 or at .01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the .01 level then at the .05 level.
\end{itemize}


%---------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
There are two kinds of errors that can be made in hypothesis testing:
\begin{itemize}
\item[(1)] a true null hypothesis can be incorrectly rejected
\item[(2)] a false null hypothesis can fail to be rejected.
\end{itemize}

The former error is called a Type I error and the latter error is called a Type II error. \\

The probability of Type I error is always equal to the level of significance that is used as the standard for rejecting
the null hypothesis; it is designated by the lowercase Greek $\alpha$ (alpha).




%---------------------------------------------------------------------------%

These two types of errors are defined in the table below.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
&True State: H0 True	& True State: H0 False\\\hline
Decision: Reject H0	& Type I error&	Correct\\
Decision: Do not Reject H0	& Correct 	&Type II error\\ \hline
\end{tabular}
\end{center}



%------------------------------------------------------------------------%
{
\noindent \textbf{Paired T test}
\large
\begin{itemize}
\item Firstly we have to compute each of the case-wise differences.
\item Then we have to compute the mean value of these differences.
\item Lastly we also have to compute the standard deviation of the differences.
\end{itemize}
}
%----------------------------------------------------------------------%
{
To test the null hypothesis that the true mean difference is zero, the procedure is as
follows:
1. Calculate the difference $(di = y_i − x_i)$ between the two observations on each pair,
making sure you distinguish between positive and negative differences.
2. Calculate the mean difference, $\bar{d}$.


3. Calculate the standard deviation of the differences, $s_d$, and use this to calculate the
standard error of the mean difference, $SE(\bar{d}) = {s_d \over \sqrt{n}}$

4. Calculate the t-statistic, which is given by $ T ={ \bar{d} \over SE(\bar{d})}$.

Under the null hypothesis, this statistic follows a t-distribution with $n − 1$ degrees of freedom.

}
%------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Formulae}
\begin{itemize}
\item The schedule of formulae that will be at the back of your examination paper will be posted on the SULIS site shortly.
\item It is advisable to familiarise yourself with the contents before the examination.
\item Please let me know as soon as possible if there are any issues with it.
\end{itemize}

%------------------------------------------------------------------------------------------------------------%

%%%% Type I and Type II errors here

%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Hypothesis Testing}
\large
The inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
\bigskip
This means that:
\begin{itemize}\item [(1)] a very unlikely event occurred or
\item[(2)] the null hypothesis is false. \end{itemize}
The inference usually made is that the null hypothesis is false. Importantly it doesn�t prove the null hypothesis to be false.

%-------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
There are two kinds of errors that can be made in hypothesis testing:
\begin{itemize}
\item[(1)] a true null hypothesis can be incorrectly rejected
\item[(2)] a false null hypothesis can fail to be rejected.
\end{itemize}
The former error is called a \textbf{\emph{Type I error}} and the latter error is called a \textbf{\emph{Type II error}}. \\ \bigskip
The probability of Type I error is always equal to the level of significance $\alpha$ (alpha) that is used as the standard for rejecting the null hypothesis .

%---------------------------------------------------------------------------%

\noindent \textbf{Type II Error}
\begin{itemize}

\item The probability of a Type II error is designated by the Greek letter beta ( $\beta$).
\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost.
\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. \item Therefore, Type I errors are generally considered more serious than Type II errors.
\item
The probability of a Type I error ($\alpha$ ) is set by the experimenter. \item There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. \item However, it increases the chance that a false null hypothesis will not be rejected, thus lowering the likelihood of Type II error.
\item
The Type I error rate is almost always set at .05 or at .01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the .01 level then at the .05 level.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
These two types of errors are defined in the table below.
\small
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
&True State: H0 True & True State: H0 False\\\hline
Decision: Reject H0 & Type I error& Correct\\
Decision: Do not Reject H0 & Correct &Type II error\\ \hline
\end{tabular}
\end{center}




%----------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and Type II errors - Die Example}
\begin{itemize}
\item Recall our die throw experiment example.
\item Suppose we perform the experiment twice with two different dice.
\item We don't not know for sure whether or not either of the dice is fair or crooked (favouring high values).
\item Suppose we get a sum of 401 from one die, and 360 from the other.
\end{itemize}


%----------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and Type II errors - Die Example}
\begin{itemize}
\item For our first dice (sum 401), we feel that it is likely that the die is crooked.
\item A Type I error describes the case when in fact that dice was fair, and what happened was just an unusual result.
\item For our second dice (sum 360), we feel that it is likely that the die is fair.
\item A Type II error describes the case when in fact that dice was crooked , favouring high values, and what happened was ,again, just an unusual result.
\end{itemize}






%------------------------------------------------------------------------------------------------------------%

%%%% Type I and Type II errors here
{
\noindent \textbf{The Paired t-test}
A paired t-test is used to compare two population means where you have two samples in
which observations in one sample can be paired with observations in the other sample.\\
\bigskip
Examples of where this might occur are:
\begin{itemize}
\item Before-and-after observations on the same subjects (e.g. students� diagnostic test
results before and after a particular module or course).
\item A comparison of two different methods of measurement or two different treatments
where the measurements/treatments are applied to the \textbf{\emph{same}} subjects.
\end{itemize}
}



%-------------------------------------------------------------------------------------------%

\noindent \textbf{The Paired t-test}
\begin{itemize}
\item We will often be required to compute the case-wise differences, the average of those differences and the standard deviation of those difference.

\item The mean difference for a set of differences between paired observations is
\[ \bar{d} = \sum d_i \over n \]

\item The computational formula for the standard deviation of the differences
between paired observations is
\[s_d = \sqrt{ {\sum d_i^2 - n\bar{d}^2 \over n-1}}\]
\item It is nearly always a small sample test.
\end{itemize}



%----------------------------------------------------------------------------------------------------%
{
\noindent \textbf{Paired T test}
\begin{itemize}
\item $\mu_d$ mean value for the population of differences.
\item The null hypothesis is that that $\mu_d = 0$
\item Given $\bar{d}$ mean value for the sample of differences, and $s_d$ standard deviation of the differences for the paired sample data, we can compute this test in the same manner as a one-sample test for the mean
\end{itemize}
}

%-------------------------------------------------------%

\noindent \textbf{Sample Size Estimation}
\textbf{New Section} This is the started of the second section of inference procedures.
\begin{itemize} \item Recall the formula for margin of error, which we shall denote $E$.
\[  E = Q_{(1-\alpha)} \times \mbox{Std. Error}\]

\item $Q_{(1-\alpha)}$ denotes the quantile that corresponds to a $1-\alpha$ confidence level. (There is quite a bit of variation in notation in this respect.)
\item Also recall that the only way to influence the margin of error is to set the sample size accordingly.

\item Sample size estimation describes the selection of a sample size $n$ such that the margin of error does not exceed a pre-determined level $E$.
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for the Mean}

\begin{itemize}

\item The margin of error does not exceed a certain threshold $E$.
\[ E \geq Q_{(1-\alpha)} \times S.E.(\bar{x}), \]

\item which can be re-expressed as
\[E \geq Q_{(1-\alpha)} \times {\sigma \over \sqrt{n} }.\]

\item Divide both sides by $\sigma \times Q_{(1-\alpha)}$.
\[ \frac{E}{\sigma Q_{(1-\alpha)}} \geq {1 \over \sqrt{n} } \]

\item Square both sides

\[ \frac{E^2}{\sigma^2 Q_{(1-\alpha)}^2} \geq {1 \over n } \]

\end{itemize}

%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for the Mean}
\begin{itemize}
\item Square both sides
\[ \frac{E^2}{\sigma^2 Q^2_{(1-\alpha)}} \geq {1 \over n } \]
\item Invert both sides, changing the direction of the relational operator.
\[ \frac{\sigma^2 Q^2_{(1-\alpha)}}{E^2} \leq n \]

\item The sample size we require is the smallest value for $n$ which satisfies this identity.
\item The sample standard deviation $s$ may be used as an estimate for $\sigma$.
\item (This formula would be provided on the exam paper).
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{SSE for the Mean: Example}
\begin{itemize}
\item An IT training company has developed a new certification program. The company wishes to estimate the average score of those who complete the program by self-study.  \item The standard deviation of the self study group is assumed to be the same as the overall population of candidates, ie. 21.2 points.
    \item How many people must be tested if the sample mean is to be in error by no more than 3 points, with 95\% confidence.
\end{itemize}


\noindent \textbf{Sample Size Estimation}
\textbf{New Section} This is the started of the second section of inference procedures.
\begin{itemize} \item Recall the formula for margin of error, which we shall denote $E$.
\[  E = Q_{(1-\alpha)} \times \mbox{Std. Error}\]

\item $Q_{(1-\alpha)}$ denotes the quantile that corresponds to a $1-\alpha$ confidence level. (There is quite a bit of variation in notation in this respect.)
\item Also recall that the only way to influence the margin of error is to set the sample size accordingly.

\item Sample size estimation describes the selection of a sample size $n$ such that the margin of error does not exceed a pre-determined level $E$.
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for the Mean}

\begin{itemize}

\item The margin of error does not exceed a certain threshold $E$.
\[ E \geq Q_{(1-\alpha)} \times S.E.(\bar{x}), \]

\item which can be re-expressed as
\[E \geq Q_{(1-\alpha)} \times {\sigma \over \sqrt{n} }.\]

\item Divide both sides by $\sigma \times Q_{(1-\alpha)}$.
\[ \frac{E}{\sigma Q_{(1-\alpha)}} \geq {1 \over \sqrt{n} } \]

\item Square both sides

\[ \frac{E^2}{\sigma^2 Q_{(1-\alpha)}^2} \geq {1 \over n } \]

\end{itemize}

%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for the Mean}
\begin{itemize}
\item Square both sides
\[ \frac{E^2}{\sigma^2 Q^2_{(1-\alpha)}} \geq {1 \over n } \]
\item Invert both sides, changing the direction of the relational operator.
\[ \frac{\sigma^2 Q^2_{(1-\alpha)}}{E^2} \leq n \]

\item The sample size we require is the smallest value for $n$ which satisfies this identity.
\item The sample standard deviation $s$ may be used as an estimate for $\sigma$.
\item (This formula would be provided on the exam paper).
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{SSE for the Mean: Example}
\begin{itemize}
\item An IT training company has developed a new certification program. The company wishes to estimate the average score of those who complete the program by self-study.  \item The standard deviation of the self study group is assumed to be the same as the overall population of candidates, ie. 21.2 points.
    \item How many people must be tested if the sample mean is to be in error by no more than 3 points, with 95\% confidence.
\end{itemize}

%--------------------------------------------------------%

\noindent \textbf{SSE for the Mean: Example}

\begin{itemize}
\item The sample size we require is the smallest value for $n$ which satisfies this identity.
\[ n \geq \frac{\sigma^2 Q^2_{(1-\alpha)}}{E^2}  \]
\item Remark: $1-\alpha$ = 0.95, therefore $Q_{(1-\alpha)}$ = 1.96. Also $E=3$ and $\sigma =21.2$.
\[ n \geq \frac{(21.2)^2 \times (1.96)^2}{3^2} \]
\item Solving, the required sample size is the smallest value of $n$ that satisfies
\[ n \geq 191.8410 \]
\item Therefore, the company needs to test 192 self-study candidates.
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for Proportions}
We can also compute appropriate sample sizes for studies based on proportions.
\begin{itemize}
\item From before; \[ E \geq Q \times S.E.(\hat{p}). \]
(For the sake of brevity, we will just use the notation $Q$ for quantile.)

\item Divide both sides by Q.

\[ E \geq Q \times \sqrt{ {\pi(1-\pi)  \over n} }. \]

\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for Proportions}
\begin{itemize}
\item Remark: $E$ must be expressed in the same form as $\pi$, either as a proportion or as a percentage.
\item Remark : The standard error is maximized at $\pi = 0.50$,which is to say $\pi(1-\pi)$ can never exceed 0.25 ( or 25\%). Therefore the standard error is maximized at $\pi = 0.50$. To make the procedure as conservative as possible, we will always use $0.25$ as our value for $\hat{p}_1 \times (1 - \hat{p}_1)$. (Equivalently 2500 for percentages).
\item If we use percentages, $\pi \times (100-\pi)$ can not exceed 2500 (i.e $ 50 \times (100-50)=2500)$.

\[ E \geq Q \times \sqrt{{2500 \over n}}. \]


\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size Estimation for proportions}

\begin{itemize}

\item Dive both sides by $Q$, the square both sides:

\[ \left({E\over Q}\right)^2 \geq {2500 \over n}. \]

\item Invert both sides, changing the direction of the relational operator, and multiply both sides by $2500$.

\[ \left({Q\over E}\right)^2 \times 2500 \leq n. \]

\item The sample size we require is the smallest value for $n$ which satisfies this identity. (The formula,  depicted two slides previously, would be provided on the exam paper, but without the maximized standard error).
\end{itemize}

%--------------------------------------------------------%

\noindent \textbf{SSE for proportions: Example}
\begin{itemize}
\item An IT journal wants to conduct a survey to estimate the true proportion of university students that own laptops.
\item The journal has decided to uses a confidence level of $95\%$, with a margin of error of $2\%$.
\item How many university students must be surveyed?
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Sample Size estimation for proportions}

\begin{itemize}
\item Confidence level = 0.95. Therefore the quantile is $Q_{(1-\alpha)} = 1.96$
\item Using the formula: \[ n \geq \left({1.96 \over 2 }\right)^2 \times 2500  \]
\item The required sample size is the smallest value for $n$ which satisfies this identity: \[ n \geq 2401  \]
\item The required sample size is therefore 2401.
\end{itemize}



%--------------------------------------------------------%

\noindent \textbf{Two Sample Inference Procedures}
\textbf{New Section} This is the started of the third section of inference procedures.
\begin{itemize}
\item Two samples are referred to as independent if the observations in one sample are not in any way related to the observations in the other. \item This is also used in cases where one randomly assign subjects to two groups, i.e. in give first group treatment A and the second group treatment B and compare the two groups.
\item Often we are interested in the difference between the mean value of some parameter for both groups.
\end{itemize}


%--------------------------------------------------------%

\noindent \textbf{Two Sample Inference Procedures}
The approach for computing a confidence interval for the difference of the means of two independent samples,  described shortly, is valid whenever the following conditions are met:

\begin{itemize}
\item Both samples are simple random samples.
\item The samples are independent.
\item Each population is at least 10 times larger than its respective sample. (Otherwise a different approach is required).
\item The sampling distribution of the difference between means is approximately normally distributed
\end{itemize}



%---------------------------------------------------------%

\noindent \textbf{Difference Of Two Means}
%-http://onlinestatbook.com/chapter8/difference_means.html
In order to construct a confidence interval, we are going to make three assumptions:

\begin{itemize}
\item The two populations have the same variance. This assumption is called the assumption of homogeneity of variance.
\item For the time being, we will use this assumption. Later on in the course, we will discuss the validity of this assumption for two given samples.
\item The populations are normally distributed.
\item Each value is sampled independently from each other value.
\end{itemize}


%---------------------------------------------------------%

\noindent \textbf{Computing the Confidence Interval}

\begin{itemize}
\item As always the first step is to compute the point estimate. For the difference of means for groups $X$ and $Y$, the point estimate is simply the difference between the two means i.e. $\bar{x} - \bar{y}$.

\item As we have seen previously, sample size has a bearing in computing both the quantile and the standard error.
For two groups, we will use the aggregate sample size ($n_x+n_y)$ to compute the quantile. (For the time being we will assume, the aggregate sample size is large ($n_x+n_y)> 30$.)

\item Lastly we must compute the standard error $S.E.(\bar{x}-\bar{y})$. The formula for computing standard error for the difference of two means, depends on whether or not the aggregate sample size is large or not. For the case that the sample size is large, we use the following formula (next slide).
\end{itemize}




%--------------------------------------------------------%

\noindent \textbf{Difference in proportions}
We can also construct a confidence interval for the difference between two sample proportions, $\pi_1 - \pi_2$. The point estimate is the difference in sample proportions for tho both groups , $\hat{p}_1- \hat{p}_2$.\\\bigskip
\textbf{Estimation Requirements}
The approach described in this lesson is valid whenever the following conditions are met:

\begin{itemize}
\item Both samples are simple random samples.
\item The samples are independent.
\item Each sample includes at least 10 successes and 10 failures.
\item The samples comprises less than 10\% of their respective populations.
\end{itemize}



%--------------------------------------------------------%


\noindent \textbf{Standard Error for Difference of Proportions}

\[ S.E. (\hat{p}_1 - \hat{p}_2) =
\sqrt{ \left[{\hat{p}_1 \times (1 - \hat{p}_1) \over n_1}\right] + \left[{\hat{p}_2 \times (1 - \hat{p}_2) \over n_2}\right] } \]
\begin{itemize}
\item $\hat{p}_1$ and $\hat{p}_2$ are the sample proportions of groups 1 and 2 respectively.
\item $n_1$ and $n_2$ are the sample sizes of groups 1 and 2 respectively.
\end{itemize}
N.B. This formula will be provided in the exam paper. Also, there is no accounting for small samples.

%--------------------------------------------------------%




\noindent \textbf{Mean Difference Between Matched Data Pairs}


The approach described in this lesson is valid whenever the following conditions are met:

\begin{itemize}
\item The data set is a simple random sample of observations from the population of interest.
\item Each element of the population includes measurements on two paired variables (e.g., x and y) such that the paired difference between x and y is: d = x - y.
\item The sampling distribution of the mean difference between data pairs (d) is approximately normally distributed.
\end{itemize}



The observed data are from the same subject or from a matched subject and are drawn from a population with a normal distribution
does not assume that the variance of both populations are equal





%---------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Computing the Case Wise Differences}
\begin{center}
\small
\begin{tabular}{|c||c|c|c|c|} \hline
Student & Before & After & Difference $(d_i)$ & $ (d_i -\bar{d})^2$ \\\hline
1 &90& 95& 5& 16 \\\hline
2 &85& 89& 4& 9 \\\hline
3 &76 &73 &-3 &4 \\\hline
4 &90& 92& 2& 1 \\\hline
5 &91 &92 &1 &0 \\\hline
6 &53 &53& 0& 1 \\\hline
7 &67 &68 &1 &4 \\\hline
8 &88 &90 &2 &9 \\\hline
9 &75 &78 &3 &16\\\hline
10 &85& 89 &4& 25 \\\hline
\end{tabular}
\end{center}



%---------------------------------------------------------------------------------------------------------------%



\noindent \textbf{Computing the Case Wise Differences}
Compute the mean difference

\[ \bar{d}  = {\sum{d_i} \over n } = { 3+6 \over 8} \]

Compute the variance of the differences.

\[ s^2_{d}  ={\sum(d_i -\bar{d})^2 \over n-1 } =  { 3+6 \over 9} \]


%---------------------------------------------------------%


\noindent \textbf{How a paired t test works}
\begin{itemize}
\item The paired t test compares two paired groups.
\item It calculates the difference between each set of pairs, and analyzes that list of differences based on the assumption that the differences in the entire population follow a Gaussian distribution.
\item First we calculate the difference between each set of pairs, keeping track of sign.
\item If the value in column B is larger, then the difference is positive.
If the value in column A is larger, then the difference is negative.
\item The t ratio for a paired t test is the mean of these differences divided by the standard error of the differences. If the t ratio is large (or is a large negative number), the P value will be small. The number of degrees of freedom equals the number of pairs minus 1. Prism calculates the P value from the t ratio and the number of degrees of freedom.
\end{itemize}

%---------------------------------------------------------%

\[ ( \bar{X} - \bar{Y} ) \pm \left[ \mbox{Quantile } \times S.E(\bar{X}-\bar{Y}) \right] \]
\begin{itemize}
\item If the combined sample size of X and Y is greater than 30, even if the individual sample sizes are less than 30, then we consider it to be a large sample.
\item The quantile is calculated according to the procedure we met in the previous class.
\end{itemize}

%---------------------------------------------------------%
\begin{itemize}
\item Assume that the mean ($\mu$) and the variance ($\sigma$) of the distribution
of people taking the drug are 50 and 25 respectively and that the mean ($\mu$)
and the variance ($\sigma$) of the distribution of people not taking the drug are
40 and 24 respectively.
\end{itemize}

%---------------------------------------------------------%

\noindent \textbf{Difference in Two means}
For this calculation, we will assume that the variances in each of the two populations are equal. This assumption is called the assumption of homogeneity of variance.

The first step is to compute the estimate of the standard error of the difference between means ().

\[ S.E.(\bar{X}-\bar{Y}) = \sqrt{\frac{s^2_x}{n_x} + \frac{s^2_y}{n_y}} \]

\begin{itemize}
\item $s^2_x$ and $s^2_x$ is the variance of both samples.
\item $n_x$ and $n_y$ is the sample size of both samples.
\end{itemize}
The degrees of freedom is $n_x + n_y -2$.

\end{document}
