
\begin{frame}
\frametitle{Information}
\begin{itemize}
\item Messages with a small difference in probability provide information also differing slightly. 
\item The information of two messges can be added if they are independent of each other.
\item Using the logarithm base 2 information provides the best possible code length in bit for this message. 
\item If the probability of a message is 0.5 the information is 1. A proper code would provide a code length of 1 bit. 
\end{itemize}
\end{frame}

%---------------------------------------------------------%


\begin{frame}

\frametitle{Redundancy Reduction}

Compression procedures intended for redundancy reduction try to adapt the internal data structure without affecting information or contents respectively. Original data will be transformed into a more efficient form to enable better usage of resources. Procedures belonging to this category are totally reversible. After decoding data will be obtained back without any difference to the original ones.

The focal point of this technology is file compression for data transfer or archiving purposes, e.g. for downloads. In this field the most important format is ZIP.

Synonymously entropy coding is in use for redundancy reduction. Strictly speaking the term entropy focusses on a smaller range of procedures. Normally it will be used for Huffman, Shannon Fano or arithmetic coding.

\end{frame}
\begin{frame}


A communication channel is a system in which the output depends
probabilistically on its input. It is characterized by a probability transition
matrix $p(y|x)$ that determines the conditional distribution of the output
given the input. For a communication channel with input X and output
Y, we can define the capacity C by




\[C = max_{p(x)} I (X; Y).\]




\end{frame}



%----------------------------------------------------------------------------%
\begin{frame}
%--http://ee.stanford.edu/~gray/it.pdf
The development of the idea of entropy of random variables and processes by
Claude Shannon provided the beginnings of information theory and of the modern
age of ergodic theory. 


We shall see that entropy and related information
measures provide useful descriptions of the long term behavior of random processes
and that this behavior is a key factor in developing the coding theorems
of information theory. 


We now introduce the various notions of entropy for random
variables, vectors, processes, and dynamical systems and we develop many
of the fundamental properties of entropy.
\end{frame}

%----------------------------------------------------------------------------%
\begin{frame}
\frametitle{Information of a Message}


In information theory, information is a value only depending on the probability for the occurrence of a particular message.



Definition: Information of a Message m


\[   I(m) = - \mbox{log}_2 P(m)\]



Corresponding to the definition above information shows the following characteristics:
\begin{itemize}
\item Increasing probability for the occurrence of a message results in decrease of information. 
\item Information always provides a positive value because probability is varying in a range of 0 to 1. 
\item The information of messages having a probability closely to 0 is very large, and for P(m) -> 0 it is infinite. 
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Huffman’s idea}
Instead of using a fixed-length code for each symbol, Huffman’s idea is to
represent a frequently occurring character in a source with a shorter code and
to represent a less frequently occurring one with a longer code. 

So for a text
source of symbols with different frequencies, the total number of bits in this
way of representation is, hopefully, significantly reduced. That is to say, the
number of bits required for each symbol on average is reduced.
\end{frame}
%-----------------------------------------------------------------%
\begin{frame}
Frequency of occurrence:\\
E A O T J Q X\\
5 5 5 3 3 2 1\\
Suppose we find a code that follows Huffman’s approach. For example, the
most frequently occurring symbol E and A are assigned the shortest 2-bit
codeword, and the lest frequently occurring symbol X is given a longer 4-bit
codeword, and so on, as below:\\
E A O T J Q X\\
10 11 000 010 011 0010 0011\\
Then the total number of bits required to encode string `EEETTJX’ is only
$2 + 2 + 2 + 3 + 3 + 3 + 4 = 19$ (bits). This is significantly fewer than
$8 \times 7 = 56$ bits when using the normal 8-bit ASCII code.
\end{frame}

%-----------------------------------------------------------------%
\begin{frame}
\large
1. Constructing a frequency table sorted in descending order.\\
2. Building a binary tree\\
Carrying out iterations until completion of a complete binary tree:\\
(a) Merge the last two items (which have the minimum frequencies) of
the frequency table to form a new combined item with a sum
frequency of the two.\\
(b) Insert the combined item and update the frequency table.\\ \bigskip
3. Deriving Huffman tree
Starting at the root, trace down to every leaf; mark ‘0’ for a left branch
and ‘1’ for a right branch.\\
4. Generating Huffman code:\\
Collecting the 0s and 1s for each path from the root to a leaf and
assigning a 0-1 codeword for each symbol.
\end{frame}
%-----------------------------------------------------------------%
\begin{frame}
\frametitle{Decoding algorithm}
The decoding process is based on the same Huffman tree. This involves the
following types of operations:
\begin{itemize}
\item We read the coded message bit by bit. Starting from the root, we follow
the bit value to traverse one edge down the the tree.
\item If the current bit is 0 we move to the left child, otherwise, to the right
child.
\item We repeat this process until we reach a leaf. If we reach a leaf, we will
decode one character and re-start the traversal from the root.
\item Repeat this read-move procedure until the end of the message.
\end{itemize}
Example : Given a Huffman-coded message,\\
111000100101111000001001000111011100000110110101,\\
what is the decoded message?
\end{frame}
\begin{frame}
\frametitle{Shannon-Fano coding}
This is another approach very similar to Huffman coding. In fact, it is the
first well-known coding method. It was proposed by C. Shannon (Bell Labs)
and R. M. Fano (MIT) in 1940.\\
The Shannon-Fano coding algorithm also uses the probability of each
symbol’s occurrence to construct a code in which each codeword can be of
different length. \\Codes for symbols with low probabilities are assigned more
bits, and the codewords of various lengths can be uniquely decoded.
\end{frame}
%-----------------------------------------------------------------%
\begin{frame}
\frametitle{Shannon-Fano algorithm}
Given a list of symbols, the algorithm involves the following steps:\\
1. Develop a frequency (or probability) table\\
2. Sort the table according to frequency (the most frequent one at the top)\\
3. Divide the table into 2 halves with similar frequency counts\\
4. Assign the upper half of the list a 0 and the lower half a 1\\
5. Recursively apply the step of division (2.) and assignment (3.) to the
two halves, subdividing groups and adding bits to the codewords until
each symbol has become a corresponding leaf on the tree.
\end{frame}
\begin{frame}
\frametitle{Importance of data compression}
Data compression techniques is motivated mainly by the need to improve
efficiency of information processing. This includes improving the following
main aspects in the digital domain:
\begin{itemize} \item storage efficiency
\item  efficient usage of transmission bandwidth
\item reduction of transmission time. \end{itemize}
\end{frame}
%-----------------------------------------------------------------%
\begin{frame}
\frametitle{Importance of data compression}
Although the cost of storage and transmission bandwidth for digital data
have dropped dramatically, the demand for increasing their capacity in many
applications has been growing rapidly ever since.\\ There are cases in which
extra storage or extra bandwidth is difficult to achieve, if not impossible.\\
Data compression as a means may make much more efficient use of existing
resources with less cost. \\ Active research on data compression can lead to
innovative new products and help provide better services.
\end{frame}
\section{Information Theory}
%-----------------------------------------------------------------------------------------------%
%---http://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/
%-----------------------------------------------------------------------------------------------%
\begin{frame}
Information theory provides us with a formula for determining the number of bits required in an optimal code even when we don't know the code. Let's first consider uniform probability distributions where the number of possible outcomes is not a power of two. Suppose we had a conventional die with six faces. The number of bits required to transmit one throw of a fair six-sided die is:$ log 6 = 2.58$. Once again,we can't really transmit a single throw in less than 3 bits, but a sequence of such throws can be transmitted using 2.58 bits on average. 
\end{frame}
%-----------------------------------------------------------------------------------------------%
\begin{frame}
\large
The optimal code in this case is complicated, but here's an approach that's fairly simple and yet does better than 3 bits/throw. Instead of treating throws individually, consider them three at a time. The number of possible three-throw sequences is $6^3= 216$. Using 8 bits we can encode a number between 0 and 255, so a three-throw sequence can be encoded in 8 bits with a little to spare; this is better than the 9 bits we'd need if we encoded each of the three throws seperately.
\end{frame}
%-----------------------------------------------------------------------------------------------%
\begin{frame}
\large
In probability terms, each possible value of the six-sided die occurs with equal probability $P=1/6$. Information theory tells us that the minmum number of bits required to encode a throw is $-log P = 2.58$. If you look back at the eight-sided die example,you'll see that in the optimal code that was described, every message had a length exactly equal to -log P bits.
\end{frame}
%-----------------------------------------------------------------------------------------------%
\begin{frame}
\large Now let's look at how to apply the formula to biased (non-uniform) probability distributions. Let the variable x range over the values to be encoded,and let P(x) denote the probability of that value occurring. The expected number of bits required to encode one value is the weighted average of the number of bits required to encode each possible value,where the weight is the probability of that value: 
\end{frame}

\end{document}
