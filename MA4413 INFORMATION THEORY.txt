% http://www.scholarpedia.org/article/Mutual_information
% http://www.scholarpedia.org/article/Entropy#Shannon_entropy
% http://www.scholarpedia.org/article/Entropy/entropy_example_3
%http://daniel-wilkerson.appspot.com/entropy.html


%---------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Symbol Sources}

Information theory traditionally deals with symbol sources that have certain properties. 
One important property is that they give out symbols that belong to a finite, predefined alphabet A. 
An alphabet can consist of for example all upper-case characters (A = {'A','B','C',..'Z',..}), all byte values (A = {0,1,..255}) or both binary digits (A = {0,1}).
As we are dealing with file compression, the symbol source is a file and the symbols (characters) are byte values from 0 to 255. 
A string or a phrase is a concatenation of symbols, for example 011101, "AAACB". Quite intuitive, right?

\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Symbol Sources}

When reading symbols from a symbol source, there is some probability for each of the symbols to appear. 
For totally random sources each symbol is equally likely, but random sources are also incompressible, and we are not interested in them here. 
Equal probabilities or not, probabilities give us a means of defining the concept of symbol self-information, i.e. the amount of information a symbol carries.

Simply, the more probable an event is, the less bits of information it contains. 
If we denote the probability of a symbol A[i] occurring as p(A[i]), the expression -log2(p(A[i])) (base-2 logarithm) gives
 the amount of information in bits that the source symbol A[i] carries. 
You can calculate base-2 logarithms using base-10 or natural logarithms if you remember that log2(n) = log(n)/log(2).
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}
A real-world example would be a comparison between two statements:

\item it is raining
\item the moon of earth has exploded.
The first case happens every once in a while (assuming we are not living in a desert area). 

Its probability may change around the world, but may be something like 0.3 during bleak autumn days. 
You would not be very surprised to hear that it is raining outside. 
It is not so for the second case. The second case would be big news, as it has never before happened, as far 
as we know. Although it seems very unlikely we could decide a very small probability for it, like 1E-30. 
The equation gives the self-information for the first case as 1.74 bits, and 99.7 bits for the second case.

\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}

\frametitle{CMessage Entropy}


So, the more probable a symbol is, the less information it carries. What about the whole message, i.e. the symbols read from the input stream? What is the information contents a specific message carries? This brings us to another concept: the entropy of a source. The measure of entropy gives us the amount of information in a message and is calculated like this: H = sum{ -p(A[i])*log2(p(A[i])) }. For completeness we note that 0*log2(0) gives the result 0 although log2(0) is not defined in itself. In essence, we multiply the information a symbol carries by the probability of the symbol and then sum all multiplication results for all symbols together.
The entropy of a message is a convenient measure of information, because it sets the lower limit for the average codeword length for a block-variable code, for example Huffman code. You can not get better compression with a statistical compression method which only considers single-symbol probabilities. The average codeword length is calculated in an analogous way to the entropy. Average code length is L = sum{-l(i)*log2(p(A[i])) }, where l(i) is the codeword length for the ith symbol in the alphabet. The difference between L and H gives an indication about the efficiency of a code. Smaller difference means more efficient code.

It is no coincidence that the entropy and average code length are calculated using very similar equations. If the symbol probabilities are not equal, we can get a shorter overall message, i.e. shorter average codeword length (i.e. compression), if we assign shorter codes for symbols that are more likely to occur. Note that entropy is only the lower limit for statistical compression systems. Other methods may perform better, although not for all files.
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}

\frametitle{Codes}
\begin{itemize}
\itam A code is any mapping from an input alphabet to an output alphabet. 
\item A code can be e.g. {a, b, c} = {0, 1, 00}, but this code is obviously not uniquely decodable. 
\item If the decoder gets a code message of two zeros, there is no way it can know whether the original message had two a's or a c.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}

\frametitle{Codes}
A code is instantaneous if each codeword (a code symbol as opposed to source symbol) in a message can be decoded as soon as it is received. 
The binary code {a, b} = {0, 01} is uniquely decodable, but it isn't instantaneous. 
You need to peek into the future to see if the next bit is 1. 
If it is, b is decoded, if not, a is decoded. The binary code {a, b, c} = {0, 10, 11} on the other hand is an instantaneous code.
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}

\frametitle{Codes}
\begin{itemize}
\item A code is a prefix code if and only if no codeword is a prefix of another codeword. 
\item A code is instantaneous if and only if it is a prefix code, so a prefix code is always a uniquely decodable instantaneous code. 
\item We only deal with prefix codes from now on. 
\item It can be proven that all uniquely decodable codes can be changed into prefix codes of equal code lengths.
\end{itemize}
\end{frame}




%---------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Lempel-Ziv 1978}

One large problem with the LZ77 method is that it does not use the coding space efficiently, i.e. there are length and offset values that never get used. If the history buffer contains multiple copies of a string, only the latest occurrance is needed, but they all take space in the offset value space. Each duplicate string wastes one offset value.
To get higher efficiency, we have to create a real dictionary. Strings are added to the codebook only once. There are no duplicates that waste bits just because they exist. Also, each entry in the codebook will have a specific length, thus only an index to the codebook is needed to specify a string (phrase). In LZ77 the length and offset values were handled more or less as disconnected variables although there is correlation. Because they are now handled as one entity, we can expect to do a little better in that regard also.

LZ78-type compressors use this kind of a dictionary. The next part of the message (the lookahead buffer contents) is searched from the dictionary and the maximum-length match is returned. The output code is an index to the dictionary. If there is no suitable entry in the dictionary, the next input symbol is sent as a literal symbol. The dictionary is updated after each symbol is encoded, so that it is possible to build an identical dictionary in the decompression code without sending additional data.

Essentially, strings that we have seen in the data are added to the dictionary. To be able to constantly adapt to the message statistics, the dictionary must be trimmed down by discarding the oldest entries. This also prevents the dictionary from becaming full, which would decrease the compression ratio. This is handled automatically in LZ77 by its use of a history buffer (a sliding window). For LZ78 it must be implemented separately. Because the decompression code updates its dictionary in sychronization with the compressor the code remains uniquely decodable.
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Run-Length Encoding}

Run length encoding also belongs to this group. If there are consecutive equal valued symbols in the input, the compressor outputs how many of them there are, and their value. Again, we must be able to identify literal bytes and compressed data. One of the RLE compressors I have seen outputs two equal symbols to indentify a run of symbols. The next byte(s) then tell how many more of these to output. If the value is 0, there are only two consecutive equal symbols in the original stream. Depending on how many bits are used to represent the value, this is the only case when the output is expanded.

Run-length encoding has been used since day one in C64 compression programs because it is very fast and very simple. Part of this is because it deals with byte-aligned data and is essentially just copying bytes from one place to another. The drawback is that RLE can only compress identical bytes into a shorter representation. On the C64 only graphics and music data contain large runs of identical bytes. Program code rarely contains more than a couple of successive identical bytes. We need something better.

That "something better" seems to be LZ77, which has been used in C64 compression programs for a long time. LZ77 can take advantage of repeating code/graphic/music data fragments and thus achieves better compression. The drawback is that practical LZ77 implementations tend to became variable-to-variable codes (more on that later) and need to handle data bit by bit, which is quite a lot slower than handling bytes.

LZ78 is not practical for C64, because the decompressor needs to create and update the dictionary. A big enough dictionary would take too much memory and updating the dictionary would need its share of processor cycles.
\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}
\frametitle{Variable-to-variable codes}

The compression algorithms in this category are mostly hybrids or concatenations of the previously described compressors. 
For example a variable-to-block code such as LZ77 followed by a statistical compressor like Huffman encoding falls into this category and is used in Zip, LHa, Gzip and many more. They use fixed, static, and adaptive statistical compression, depending on the program and the compression level selected.
Randomly concatenating algorithms rarely produces good results, so you have to know what you are doing and what kind of files you are compressing. 
Whenever a novice asks the usual question: 'What compression program should I use?', they get the appropriate response: 'What kind of data are you compressing?'

Borrowed from Tom Lane's article in comp.compression:
It's hardly ever worthwhile to take the compressed output of one compression method and shove it through another compression method. 
Especially not if the second method is a general-purpose compressor that doesn't have specific knowledge of the first compression step. Compression is effective in direct proportion to the extent that it eliminates obvious patterns in the data. So if the first compression step is any good, it will leave little traction for the second step. Combining multiple compression methods is only helpful when the methods are specifically chosen to be complementary.

A small sidetrack I want to take:
This also brings us conveniently to another truth in lossless compression. 
There isn't a single compressor which would be able to losslessly compress all possible files (you can see the comp.compression FAQ for information about the counting proof). It is our luck that we are not interested in compressing all files. We are only interested in compressing a very small subset of all files. The more accurately we can describe the files we would encounter, the better. This is called modelling, and it is what all compression programs do and must do to be successful.

Audio and graphics compression algorithm may assume a continuous signal, and a text compressor may assume that there are 
repeated strings in the data. If the data does not match the assumptions (the model), the algorithm usually expands the data instead of compressing it.

% http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_1.html#fn1

\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}
reference 
Some of this discussion is drawn from Communication Systems Engineering, John G. Proakis and Masoud Salehi, Prentice-Hall (1994), ISBN 0-13-158932-6.

Definition of the logarithmic function:

By way of an introduction to logarithms, you may or may not recall, that if we take


this means
 
Thus, logarithms have the following important property:


\end{frame}
%---------------------------------------------------------------------------------------------%
\begin{frame}  
  
 Examples of logarithms:
 

 
 
(or a value of 3 bels = 30dB)
 
 
(or a value of 2.30 bels = 23.0dB)
 
 
the "base-changing" rule
 
 
 
an application of the base changing rule

%---------------------------------------------------------------------------------------------%


Self-Information - "Entropy"

 
As the first step in finding a measure of information, consider an information source with a series of ordered outputs: reference


where the output  is    most-likely and    is least-likely -- e.g.   might be, for example, the weather condition and air pollution level in a given city and on a certain day or, perhaps, the outcome of a particular athletic event or …… 
  
 
A measure of "information" should satisfy the following conditions

 
The information content of an output  depends only on the probability of   occurring  -- i.e. --  and not on the value of  .  We denote this function by  and call it the self-information of the output.
                      
                  Note that   
 
Self-information is a continuous function of 

 
Self-information is a decreasing function of  

 
If    , then     .

 
Only the "logarithmic" function definition satisfies these essential properties and thus self-information may be written




Therefore, the information revealed by a particular source output is the "weighted' average of the self-information of each of the various outputs --

 
 
        

which is usually called (but be careful see caution ) the entropy of the sourc


Recall that entropy is given by the general formula:      


Example 3:

Consider a discrete memoryless information source with binary output alphabet with the
respective probabilities p and (1 - p):



Note the maximum entropy occurs when the probabilities are equal!
  
Example 4 : 

Consider another source sampled at a rate of 8,000 samples per second (with implies that the souce has 
a bandwidth of 4 kHz and, thus, is sampled at the Nyquist rate). The resulting sample sequence can be 
approximated as a discrete memoryless information source with a five element output alphabet with output probabilities        .


  
 
Therefore the source produces information at a rate of 15 kbits per second. 

